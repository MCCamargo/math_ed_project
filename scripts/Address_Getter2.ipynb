{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import googlemaps\n",
    "import concurrent.futures\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Google Maps API client\n",
    "def load_api_key(filepath):\n",
    "    \"\"\"Reads API key from a text file.\"\"\"\n",
    "    with open(filepath, \"r\") as file:\n",
    "        return file.read().strip()\n",
    "\n",
    "API_KEY = load_api_key(\"Google_API_Key.txt\")  #Replace with your API key (not provided in git repo)\n",
    "gmaps = googlemaps.Client(key=API_KEY)\n",
    "\n",
    "def get_location_details_google(row):\n",
    "    \"\"\"Fetch latitude, longitude, full street address, ZIP code, and county from Google Maps API.\"\"\"\n",
    "    \n",
    "    if \"District Name\" in row:\n",
    "        query = f\"{row['School Name']}, {row['District Name']}, {state}, High School\"\n",
    "    else:\n",
    "        query = f\"{row['School Name']}, {state}, High School\"\n",
    "\n",
    "    \n",
    "    try:\n",
    "        geocode_result = gmaps.geocode(query)\n",
    "        if geocode_result:\n",
    "            location = geocode_result[0][\"geometry\"][\"location\"]\n",
    "            lat, lon = location[\"lat\"], location[\"lng\"]\n",
    "            \n",
    "            \n",
    "            # Extract address components\n",
    "            address_components = geocode_result[0].get(\"address_components\", [])\n",
    "            street_number, street_name, zip_code, county, city = \"\", \"\", \"N/A\", \"N/A\", \"N/A\"\n",
    "            \n",
    "            for component in address_components:\n",
    "                types = component[\"types\"]\n",
    "                if \"street_number\" in types:\n",
    "                    street_number = component[\"long_name\"]\n",
    "                if \"route\" in types:\n",
    "                    street_name = component[\"long_name\"]\n",
    "                if \"postal_code\" in types:\n",
    "                    zip_code = component[\"long_name\"]\n",
    "                if \"administrative_area_level_2\" in types:\n",
    "                    county = component[\"long_name\"]\n",
    "                if \"locality\" in types:  # This is for city\n",
    "                    city = component[\"long_name\"]\n",
    "            \n",
    "                    \n",
    "            full_street_address = f\"{street_number} {street_name}\".strip()\n",
    "            return pd.Series([lat, lon, full_street_address,  zip_code, county, city])\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching details for {query}: {e}\")\n",
    "    \n",
    "    return pd.Series([\"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\"])\n",
    "\n",
    "\n",
    "def fetch_missing_county_names(df, gmaps, state):\n",
    "    \"\"\"Fill in missing or 'N/A' county names using Google Maps API.\"\"\"\n",
    "\n",
    "    def get_county_only(row):\n",
    "        if pd.notna(row['county_name']) and row['county_name'].strip().upper() != \"N/A\":\n",
    "            return row['county_name'].strip()  # Already exists and is valid\n",
    "\n",
    "        if pd.isna(row[\"latitude\"]) or pd.isna(row[\"longitude\"]):\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            result = gmaps.reverse_geocode((row[\"latitude\"], row[\"longitude\"]))\n",
    "            for component in result[0].get(\"address_components\", []):\n",
    "                if \"administrative_area_level_2\" in component[\"types\"]:\n",
    "                    return component[\"long_name\"].strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error for index {row.name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    df[\"county_name\"] = df.apply(get_county_only, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "    df[\"county_name\"] = df.apply(get_county_only, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_census_tract(lat, lon):\n",
    "    \"\"\"Fetch the census tract for given latitude and longitude using the Census Geocoder API.\"\"\"\n",
    "    # url = f\"https://geocoding.geo.census.gov/geocoder/geographies/coordinates?x={lon}&y={lat}&benchmark=Public_AR_2020&vintage=Census2020_Census2020&layers=10&format=json\"\n",
    "    url = f\"https://geocoding.geo.census.gov/geocoder/geographies/coordinates?x={lon}&y={lat}&benchmark=Public_AR_Current&vintage=Current_Current&layers=10&format=json\"\n",
    "\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)  # Set timeout to avoid hanging requests\n",
    "        response.raise_for_status()  # Raise an error for HTTP issues\n",
    "        data = response.json()\n",
    "        \n",
    "        geographies = data.get('result', {}).get('geographies', {})\n",
    "        if 'Census Block Groups' in geographies:\n",
    "            tract_id = str(geographies['Census Block Groups'][0].get('TRACT', 'Not found'))\n",
    "            return tract_id.zfill(6)  # Ensure 6-digit format\n",
    "        return 'No data'\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "\n",
    "\n",
    "def add_missing_census_tracts(df, max_workers=5):\n",
    "    \"\"\"Only fetch census tracts for rows where Tract == '000000'.\"\"\"\n",
    "    \n",
    "    # Filter rows with bad tract data\n",
    "    mask = df['Tract'] == \"000000\"\n",
    "    df_missing = df[mask].copy()\n",
    "\n",
    "    if df_missing.empty:\n",
    "        print(\"No missing tracts to update.\")\n",
    "        return df\n",
    "    \n",
    "    print(f\"Updating {len(df_missing)} missing tracts...\")\n",
    "\n",
    "    # Fetch the good tracts\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        new_tracts = list(executor.map(get_census_tract, df_missing['latitude'], df_missing['longitude']))\n",
    "\n",
    "    # Update only those rows in the original dataframe\n",
    "    df.loc[mask, 'Tract'] = new_tracts\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def batch_census_tract(indf):\n",
    "\n",
    "    #Takes in data frame, indf which has column heads, among others, \"Address\", \"City\", \"State\", and \"Zip\" and batch looks up census tract info.\n",
    "\n",
    "    indf[\"Unique ID\"] = indf.index\n",
    "\n",
    "    df2=indf[[\"address\",\"city_name\",\"state_name\",\"zip_code\",\"Unique ID\"]] #grab just relevant columns\n",
    "\n",
    "    # Remove rows where 'Address' is NaN or an empty string\n",
    "    df2 = df2[df2['address'].notna() & (df2['address'] != '')]\n",
    "    df2 =  df2[df2['city_name'].notna() & (df2['city_name'] != '')]\n",
    "    df2.columns = [\"Street Address\",\"City\",\"State\",\"Zip\",\"Unique ID\"]\n",
    "    df2 = df2[[\"Unique ID\"] + [col for col in df2.columns if col != \"unique ID\"]] #Unique ID needs to be first column\n",
    "\n",
    "    csv_buffer = io.StringIO()\n",
    "    df2.to_csv(csv_buffer, index=False, header=False, quoting=1)  # quoting=1 forces proper quoting\n",
    "    csv_buffer.seek(0)  # Move to the start of the buffer\n",
    "\n",
    "    # API Endpoint\n",
    "    url = \"https://geocoding.geo.census.gov/geocoder/geographies/addressbatch\"\n",
    "    # url = \"https://geocoding.geo.census.gov/geocoder/geographies/addressbatch?benchmark=Public_AR_2020&vintage=Census2020_Census2020\"\n",
    "\n",
    "    # API Request with in-memory file\n",
    "    files = {\"addressFile\": (\"addresses.csv\", csv_buffer.getvalue())}\n",
    "    data = {\"benchmark\": \"4\", \"vintage\": \"4\"}\n",
    "\n",
    "    response = requests.post(url, files=files, data=data)\n",
    "\n",
    "    # Read response into a new DataFrame (FIXED!)\n",
    "    result_buffer = io.StringIO(response.text)\n",
    "\n",
    "    result_df = pd.read_csv(\n",
    "    result_buffer,\n",
    "    header=None,\n",
    "    quotechar='\"',  # This ensures that quoted fields (like addresses and lat/lon) stay intact\n",
    "    names=[\n",
    "        \"Unique ID\", \"Street Address\", \"Match Status\", \"Match Type\", \n",
    "        \"Standardized Address\", \"Coordinates\", \"TigerLine ID\", \"Side\", \n",
    "        \"State FIPS\", \"County FIPS\", \"Tract\", \"Block\"\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    # Split Latitude and Longitude.\n",
    "    result_df[['Longitude', 'Latitude']] = result_df['Coordinates'].str.split(',', expand=True)\n",
    "    result_df.drop(columns=['Coordinates'], inplace=True)  # Drop combined colum\n",
    "\n",
    "    result_df[\"Tract\"] = result_df[\"Tract\"].fillna(0).astype(int)  # Fill NaNs with 0 and then convert\n",
    "    result_df.loc[result_df[\"Tract\"]==0, \"Tract\"] = \"\" \n",
    "\n",
    "    #Fill in updated county code when possible\n",
    "    result_df = result_df.set_index(\"Unique ID\")\n",
    "    indf[\"county_code\"] = indf[\"county_code\"].fillna(result_df[\"County FIPS\"])\n",
    "    result_df = result_df.reset_index()\n",
    "\n",
    "    #Drop Unnec. Columns\n",
    "    result_df = result_df.drop(columns = [\"Match Status\", \"Match Type\", \"Standardized Address\", \"Side\",\"State FIPS\",\"TigerLine ID\",\"Block\",\"County FIPS\",\"Latitude\",\"Longitude\"])\n",
    "\n",
    "    #Merge Results\n",
    "    indf = indf.merge(result_df,on=\"Unique ID\", how = \"left\")\n",
    "    indf = indf.drop(columns = [\"Unique ID\"])\n",
    "    indf[\"Tract\"] = indf[\"Tract\"].fillna(\"\")\n",
    "\n",
    "    #Pad Tract ID to 6 digit format\n",
    "    indf[\"Tract\"] = indf[\"Tract\"].astype(str).str.zfill(6)\n",
    "\n",
    "    #return modified input df\n",
    "    return indf\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_637475/1884623926.py:173: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  result_df.loc[result_df[\"Tract\"]==0, \"Tract\"] = \"\"\n",
      "/tmp/ipykernel_637475/1884623926.py:177: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  indf[\"county_code\"] = indf[\"county_code\"].fillna(result_df[\"County FIPS\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 119 missing tracts...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/ops/array_ops.py:218\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 218\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(left, right)\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/computation/expressions.py:242\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_numexpr:\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;66;03m# error: \"None\" not callable\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate(op, op_str, a, b)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/computation/expressions.py:131\u001b[0m, in \u001b[0;36m_evaluate_numexpr\u001b[0;34m(op, op_str, a, b)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 131\u001b[0m     result \u001b[38;5;241m=\u001b[39m _evaluate_standard(op, op_str, a, b)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/computation/expressions.py:73\u001b[0m, in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b)\u001b[0m\n\u001b[1;32m     72\u001b[0m     _store_test_result(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op(a, b)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'float' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[243], line 64\u001b[0m\n\u001b[1;32m     57\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mmerge(dfcensus, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munique_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m---> 64\u001b[0m df \u001b[38;5;241m=\u001b[39m build_csv(input_ed_csv,input_census_csv,input_county_csv,state,output_csv)\n\u001b[1;32m     65\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[0;32mIn[243], line 55\u001b[0m, in \u001b[0;36mbuild_csv\u001b[0;34m(input_ed_csv, input_census_csv, input_county_csv, state, output_csv)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#only then do individual census geoprocessing, on the stragglers\u001b[39;00m\n\u001b[1;32m     52\u001b[0m df \u001b[38;5;241m=\u001b[39m add_missing_census_tracts(df)\n\u001b[0;32m---> 55\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munique_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcounty_code\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTract\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     57\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mmerge(dfcensus, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munique_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, other)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/arraylike.py:186\u001b[0m, in \u001b[0;36mOpsMixin.__add__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__add__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__add__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m    100\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    Get Addition of DataFrame and other, column-wise.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    moose     3.0     NaN\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arith_method(other, operator\u001b[38;5;241m.\u001b[39madd)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/series.py:6135\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_arith_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[1;32m   6134\u001b[0m     \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_align_for_op(other)\n\u001b[0;32m-> 6135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m base\u001b[38;5;241m.\u001b[39mIndexOpsMixin\u001b[38;5;241m.\u001b[39m_arith_method(\u001b[38;5;28mself\u001b[39m, other, op)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/base.py:1382\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1379\u001b[0m     rvalues \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(rvalues\u001b[38;5;241m.\u001b[39mstart, rvalues\u001b[38;5;241m.\u001b[39mstop, rvalues\u001b[38;5;241m.\u001b[39mstep)\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1382\u001b[0m     result \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39marithmetic_op(lvalues, rvalues, op)\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(result, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/ops/array_ops.py:283\u001b[0m, in \u001b[0;36marithmetic_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    279\u001b[0m     _bool_arith_check(op, left, right)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;66;03m# error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\u001b[39;00m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(left, right, op)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/ops/array_ops.py:227\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cmp \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    221\u001b[0m         left\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(right, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m\n\u001b[1;32m    222\u001b[0m     ):\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;66;03m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;66;03m#  incorrectly, see GH#32047\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m         result \u001b[38;5;241m=\u001b[39m _masked_arith_op(left, right, op)\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/ops/array_ops.py:163\u001b[0m, in \u001b[0;36m_masked_arith_op\u001b[0;34m(x, y, op)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# See GH#5284, GH#5035, GH#19448 for historical reference\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m--> 163\u001b[0m         result[mask] \u001b[38;5;241m=\u001b[39m op(xrav[mask], yrav[mask])\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(y):\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "state = \"Massachusetts\"  # State\n",
    "input_ed_csv = \"/home/mark/Documents/Erdos/Proj/math_ed_project/data/MA23/MA23_ed_data.csv\" # Input education data file\n",
    "input_census_csv = \"/home/mark/Documents/Erdos/Proj/math_ed_project/data/census_data/MA/23/MA23_census_data.csv\" #Input census data file\n",
    "input_county_csv = \"~/Documents/Erdos/Proj/math_ed_project/data/county_codes.csv\" #Path to County Codes\n",
    "output_csv = \"/home/mark/Documents/Erdos/Proj/math_ed_project/data/MA23/MA23_Combined_Census_Education.csv\"  # Output file\n",
    "\n",
    "\n",
    "def build_csv(input_ed_csv,input_census_csv,input_county_csv,state,output_csv):\n",
    "    #Takes input_ed_csv, input_census_csv, and input_county_csv, and combines into single csv.\n",
    "    # *_csv arguments are all filepaths to csv file.\n",
    "    # state is a string specifying which state this applies to.\n",
    "   \n",
    "\n",
    "    #Import and format county code data\n",
    "    countydf=pd.read_csv(input_county_csv)\n",
    "    countydf[\"state_name\"] = countydf[\"state_name\"].str.strip()\n",
    "    countydf[\"county_name\"] = countydf[\"county_name\"].str.strip()\n",
    "    countydf[\"county_code\"]=countydf[\"county_code\"].astype(str)\n",
    "\n",
    "    # Read ed_CSV into Pandas DataFrame\n",
    "    ext = input_ed_csv.split('.')[-1] #Get the file type, since they're not all csv files.\n",
    "    if ext == \"csv\":\n",
    "        df = pd.read_csv(input_ed_csv)\n",
    "    else:\n",
    "        df = df = pd.read_excel(input_ed_csv)\n",
    "\n",
    "    #Can't work with entries with no school name\n",
    "    df = df.dropna(subset=[\"School Name\"])\n",
    "\n",
    "\n",
    "    #Read census_CSV into data frame. Make unique ID a string, which ends up being convenient.\n",
    "    dfcensus = pd.read_csv(input_census_csv)\n",
    "    dfcensus[\"unique_id\"]=dfcensus[\"unique_id\"].astype(str)\n",
    "\n",
    "    #Fetch Geo data using google maps API\n",
    "  # Apply the function to get location details and add the resulting columns to the DataFrame\n",
    "    df[\"state_name\"] = state\n",
    "    df[[\"latitude\", \"longitude\", \"address\", \"zip_code\", \"county_name\", \"city_name\"]] = df.apply(get_location_details_google, axis=1)\n",
    "\n",
    "    #For whatever reason google doesn't succeed at grabbing all the county names at first pass.\n",
    "    fetch_missing_county_names(df,gmaps,state)\n",
    "    \n",
    "    #Load County ID data into df\n",
    "    df = df.merge(countydf, on=[\"state_name\", \"county_name\"], how=\"left\")\n",
    "\n",
    "\n",
    "    #First do batch census geoprocessing wherever possible\n",
    "    df = batch_census_tract(df)\n",
    "\n",
    "\n",
    "    #only then do individual census geoprocessing, on the stragglers\n",
    "    df = add_missing_census_tracts(df)\n",
    "\n",
    "      \n",
    "    df[\"unique_id\"] = df[\"county_code\"] + df[\"Tract\"]\n",
    "\n",
    "    df = df.merge(dfcensus, on=\"unique_id\", how=\"left\")\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "df = build_csv(input_ed_csv,input_census_csv,input_county_csv,state,output_csv)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (df[df[\"American Indian and Alaska Native alone (estimate)\"].isna()].shape)[0] <20:\n",
    "    df = df.dropna(subset = [\"American Indian and Alaska Native alone (estimate)\"])\n",
    "    df.to_csv(output_csv, index=False)\n",
    "else:\n",
    "    print(\"false\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104           NaN\n",
       "129    5073950507\n",
       "285    5147480401\n",
       "Name: unique_id, dtype: object"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df[df[\"American Indian and Alaska Native alone (estimate)\"].isna()].unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Texas\n",
       "1       Texas\n",
       "2       Texas\n",
       "3       Texas\n",
       "4       Texas\n",
       "        ...  \n",
       "1705    Texas\n",
       "1706    Texas\n",
       "1707    Texas\n",
       "1708    Texas\n",
       "1709    Texas\n",
       "Name: state_name_x, Length: 1702, dtype: object"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
